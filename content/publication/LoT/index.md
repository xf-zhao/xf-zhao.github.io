---
title: Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
authors: 
- Xufeng Zhao
- Mengdi Li
- Wenhao Lu
- Cornelius Weber
- Jae Hee Lee
- Kun Chu
- Stefan Wermter
journal: 
- COLING 2024 (oral)
summary: We propose LoT (Logical Thoughts), a framework that improves large language modelsâ€™ reasoning at inference time by applying symbolic logic to verify and correct their step-by-step thought process. LoT enhances performance on diverse reasoning tasks and reduces hallucinations.
tags:
- LLMs
date: "2024-10-27T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: ""

image:
  caption: 
  focal_point: Smart

# links:
# - icon: twitter
#   icon_pack: fab
#   name: Follow
#   url: https://twitter.com/georgecushen
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---
